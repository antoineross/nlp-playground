{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pool the individual embeddings to create a vector representation for whole sentences. These embeddings can then be used to find similar documents in the corpus by computing the dot-product similarity (or some other similarity metric) between each embedding and **returning the documents with the greatest overlap**.\n",
    "\n",
    "In this section we’ll use embeddings to develop a semantic search engine. These search engines offer several advantages over conventional approaches that are based on matching keywords in a query with the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's filter out the data that can be considered as noise. \n",
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All we need for our search engine is [\"title\", \"body\", \"html_url\", \"comments\"]. Let's remove the rest.\n",
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comments that we got for now is a list of comments for each issue.      \n",
    "Let's create one row for each comment using explode() function so that each row consists of an (html_url, title, body, comment) tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's set the format to Pandas DataFrame so we can visualize the data:\n",
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]\n",
    "# Inspect the first row:\n",
    "df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                    Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explode the comments:\n",
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter out short comments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we've exploded the comments, we go back to datasets:\n",
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88798e14d5d24647939f590d070349fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b70a50be2242caa8deb16afa9d048c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a new column that contain the amount of words per comment first:\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")\n",
    "\n",
    "# If words are > 15, we keep x.\n",
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having cleaned up our dataset a bit, let’s concatenate the issue title, description, and comments together in a new text column. As usual, we’ll write a simple function that we can pass to Dataset.map():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a338b72e88ee487daf18555ec844bfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/datasets/issues/2943',\n",
       " 'title': 'Backwards compatibility broken for cached datasets that use `.filter()`',\n",
       " 'comments': \"Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?\",\n",
       " 'body': '## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n',\n",
       " 'comment_length': 50,\n",
       " 'text': 'Backwards compatibility broken for cached datasets that use `.filter()` \\n ## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n \\n Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don\\'t use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)\n",
    "comments_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating text embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34fce8535e4456a9be4ca400983b448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 16:06:43.745674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 16:06:45.164406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94981c814d19418cbc48ca724d1a3cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 16:07:36.045967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-26 16:07:36.055237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-26 16:07:36.055315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-26 16:07:36.057018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-26 16:07:36.057089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-26 16:07:36.057138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-26 16:07:43.851180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-26 16:07:43.851549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-26 16:07:43.851581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-26 16:07:43.851775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-26 16:07:43.852112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5420 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-08-26 16:08:14.936464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-26 16:08:18.217078: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93778944 exceeds 10% of free system memory.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFMPNetModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True) # Weights from PT to TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’d like to represent each entry in our GitHub issues corpus as a single vector, so we need to “pool” or average our token embeddings in some way. One popular approach is to perform CLS pooling on our model’s outputs, where we simply collect the last hidden state for the special [CLS] token. The following function does the trick for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "# Next, we’ll create a helper function that will tokenize a list of documents, \n",
    "# place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs:\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"tf\"\n",
    "    )\n",
    "    encoded_input = {k: v for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feeding it the first text entry in our corpus and inspecting the output shape:\n",
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 16:27:21.389866: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93778944 exceeds 10% of free system memory.\n",
      "/home/antoine/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388f0465cd3042de827fc68d93c304d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We’ve converted the first entry in our corpus into a 768-dimensional vector.\n",
    "# Let's create a new embeddings column now:\n",
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).numpy()[0]} # FAISS requires numpy arrays.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using FAISS for efficient similarity search**       \n",
    "Facebook AI Similarity Search (FAISS) is a library that provides efficient algorithms to quickly search and cluster embedding vectors. This will help us search over our dataset of embeddings. The basic idea behind FAISS is to create a special data structure called an index that allows one to find which embeddings are similar to an input embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. A community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. Note that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# We create a FAUSS index using Datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m embeddings_dataset\u001b[39m.\u001b[39;49madd_faiss_index(column\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39membeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/datasets/arrow_dataset.py:5693\u001b[0m, in \u001b[0;36mDataset.add_faiss_index\u001b[0;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose, dtype)\u001b[0m\n\u001b[1;32m   5639\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Add a dense index using Faiss for fast retrieval.\u001b[39;00m\n\u001b[1;32m   5640\u001b[0m \u001b[39mBy default the index is done over the vectors of the specified column.\u001b[39;00m\n\u001b[1;32m   5641\u001b[0m \u001b[39mYou can specify `device` if you want to run it on GPU (`device` must be the GPU index).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5690\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[1;32m   5691\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5692\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformatted_as(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, columns\u001b[39m=\u001b[39m[column], dtype\u001b[39m=\u001b[39mdtype):\n\u001b[0;32m-> 5693\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madd_faiss_index(\n\u001b[1;32m   5694\u001b[0m         column\u001b[39m=\u001b[39;49mcolumn,\n\u001b[1;32m   5695\u001b[0m         index_name\u001b[39m=\u001b[39;49mindex_name,\n\u001b[1;32m   5696\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m   5697\u001b[0m         string_factory\u001b[39m=\u001b[39;49mstring_factory,\n\u001b[1;32m   5698\u001b[0m         metric_type\u001b[39m=\u001b[39;49mmetric_type,\n\u001b[1;32m   5699\u001b[0m         custom_index\u001b[39m=\u001b[39;49mcustom_index,\n\u001b[1;32m   5700\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   5701\u001b[0m         train_size\u001b[39m=\u001b[39;49mtrain_size,\n\u001b[1;32m   5702\u001b[0m         faiss_verbose\u001b[39m=\u001b[39;49mfaiss_verbose,\n\u001b[1;32m   5703\u001b[0m     )\n\u001b[1;32m   5704\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/datasets/search.py:480\u001b[0m, in \u001b[0;36mIndexableMixin.add_faiss_index\u001b[0;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Add a dense index using Faiss for fast retrieval.\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[39mThe index is created using the vectors of the specified column.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[39mYou can specify `device` if you want to run it on GPU (`device` must be the GPU index, see more below).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39m    faiss_verbose (`bool`, defaults to False): Enable the verbosity of the Faiss index.\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    479\u001b[0m index_name \u001b[39m=\u001b[39m index_name \u001b[39mif\u001b[39;00m index_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m column\n\u001b[0;32m--> 480\u001b[0m faiss_index \u001b[39m=\u001b[39m FaissIndex(\n\u001b[1;32m    481\u001b[0m     device\u001b[39m=\u001b[39;49mdevice, string_factory\u001b[39m=\u001b[39;49mstring_factory, metric_type\u001b[39m=\u001b[39;49mmetric_type, custom_index\u001b[39m=\u001b[39;49mcustom_index\n\u001b[1;32m    482\u001b[0m )\n\u001b[1;32m    483\u001b[0m faiss_index\u001b[39m.\u001b[39madd_vectors(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m, column\u001b[39m=\u001b[39mcolumn, batch_size\u001b[39m=\u001b[39mbatch_size, train_size\u001b[39m=\u001b[39mtrain_size, faiss_verbose\u001b[39m=\u001b[39mfaiss_verbose\n\u001b[1;32m    485\u001b[0m )\n\u001b[1;32m    486\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indexes[index_name] \u001b[39m=\u001b[39m faiss_index\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/datasets/search.py:247\u001b[0m, in \u001b[0;36mFaissIndex.__init__\u001b[0;34m(self, device, string_factory, metric_type, custom_index)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfaiss_index \u001b[39m=\u001b[39m custom_index\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_faiss:\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNote that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: You must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. A community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. Note that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available."
     ]
    }
   ],
   "source": [
    "# We create a FAUSS index using Datasets\n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now perform queries on this index by doing a nearest neighbor lookup with the Dataset.get_nearest_examples() function. \n",
    "# Let’s test this out by first embedding a question as follows:\n",
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like with the documents, we now have a 768-dimensional vector representing the query, which we can compare against the whole corpus to find the most similar embeddings:\n",
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset.get_nearest_examples() function returns a tuple of scores that rank the overlap between the query and the document, and a corresponding set of samples (here, the 5 best matches). Let’s collect these in a pandas.DataFrame so we can easily sort them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "# Now we can iterate over the first few rows to see how well our query matched the available comments:\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
